# -*- coding: utf-8 -*-
"""seamless_m4t_finetuning_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KDu5mHGKoAuF2dcQMqmIIga7h2ePGgP2

# SeamlessM4T Medium Finetuning

Using v1 medium model with correct model class.

- Hindi → Urdu | Hindi → Odia | Odia → Urdu
- **Metrics**: BLEU, ChrF, COMET, WER, MOS

---
## 1. Installation
"""

!nvidia-smi

# Install with specific datasets version that supports loading scripts
!pip install -q git+https://github.com/huggingface/transformers.git sentencepiece
!pip install -q accelerate torchaudio soundfile
!pip install -q "datasets<3.0.0"  # Older version that still supports loading scripts
!pip install -q sacrebleu jiwer
print("Done!")

!pip install -q unbabel-comet
print("COMET installed!")

print("="*50)
print("RESTART RUNTIME: Runtime -> Restart runtime")
print("Then continue from Section 2")
print("="*50)



import random
import numpy as np

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

print(f"Random seed set to {SEED}")

"""---
## 2. Setup (After Restart)
"""

import os, json, torch, torchaudio, time
from pathlib import Path
from tqdm.notebook import tqdm
from typing import List, Dict
from IPython.display import Audio, display

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}, PyTorch: {torch.__version__}")

DATA_DIR, SAVE_DIR, MOS_DIR = "/content/data", "/content/checkpoints", "/content/mos"
for d in [DATA_DIR, SAVE_DIR, MOS_DIR]:
    os.makedirs(d, exist_ok=True)

import datasets
print(f"datasets version: {datasets.__version__}")


DEBUG_LOG_PATH = r"d:\\EACL\\.cursor\\debug.log"  # primary (Cursor workspace)
DEBUG_LOG_PATH_FALLBACK = "/content/debug.log"      # Colab-safe
DEBUG_SESSION_ID = "debug-session"
DEBUG_RUN_ID = os.environ.get("EACL_RUN_ID", "precheck1")

# Best-effort clear fallback log each run (Colab)
try:
    if os.path.exists(DEBUG_LOG_PATH_FALLBACK):
        os.remove(DEBUG_LOG_PATH_FALLBACK)
except Exception:
    pass

def _dbglog(hypothesisId, location, message, data=None):
    """Append one NDJSON line to a log file, and always print it (Colab-friendly)."""
    payload = {
        "sessionId": DEBUG_SESSION_ID,
        "runId": DEBUG_RUN_ID,
        "hypothesisId": hypothesisId,
        "location": location,
        "message": message,
        "data": data or {},
        "timestamp": int(time.time() * 1000),
    }
    line = json.dumps(payload, ensure_ascii=False)
    wrote = False
    used_path = None
    for path in (DEBUG_LOG_PATH, DEBUG_LOG_PATH_FALLBACK):
        try:
            d = os.path.dirname(path)
            if d:
                os.makedirs(d, exist_ok=True)
            with open(path, "a", encoding="utf-8") as f:
                f.write(line + "\n")
            wrote = True
            used_path = path
            break
        except Exception:
            continue
    print("DBGLOG", line)
    return wrote, used_path

wrote, used = _dbglog(
    "H0",
    "seamless_m4t_finetuning_colab.ipynb:setup",
    "dbglog_ready",
    {
        "os_name": os.name,
        "cwd": os.getcwd(),
        "primary": DEBUG_LOG_PATH,
        "fallback": DEBUG_LOG_PATH_FALLBACK,
    },
)
print(f"[dbglog] wrote={wrote} used={used} fallback_exists={os.path.exists(DEBUG_LOG_PATH_FALLBACK)}")

"""---
## 3. Load SeamlessM4T Medium (v1)
"""

from transformers import AutoProcessor, SeamlessM4TModel

MODEL_ID = "facebook/hf-seamless-m4t-medium"  # v1 medium - 1.2B params

# Use fp32 for training stability (fp16 causes NaN in loss computation)
# Training with fp16 cross-entropy is numerically unstable for SeamlessM4T
DTYPE = torch.float32

print(f"Loading {MODEL_ID} (dtype={DTYPE})...")
processor = AutoProcessor.from_pretrained(MODEL_ID)
model = SeamlessM4TModel.from_pretrained(MODEL_ID, torch_dtype=DTYPE).to(device)
model.eval()
print(f"Loaded! Params: {sum(p.numel() for p in model.parameters()):,}")

LANG = {"hin": "hin", "urd": "urd", "ory": "ory"}
FLEURS = {"hin": "hi_in", "urd": "ur_pk", "ory": "or_in"}

"""---
## 4. FLEURS Data
"""

from datasets import load_dataset

def load_data(src, tgt, split, max_n=None):
    # Load FLEURS dataset
    src_ds = load_dataset("google/fleurs", FLEURS[src], split=split, cache_dir=DATA_DIR)
    tgt_ds = load_dataset("google/fleurs", FLEURS[tgt], split=split, cache_dir=DATA_DIR)
    tgt_map = {x["id"]: x for x in tgt_ds}
    samples = []
    for item in tqdm(src_ds, desc=split):
        if item["id"] in tgt_map:
            samples.append({"id": item["id"], "audio": item["audio"]["array"], "sr": item["audio"]["sampling_rate"],
                           "src_text": item["transcription"], "tgt_text": tgt_map[item["id"]]["transcription"]})
            if max_n and len(samples) >= max_n: break
    print(f"{split}: {len(samples)}")
    return samples

SRC, TGT = "ory", "urd"
train_data = load_data(SRC, TGT, "train")
val_data = load_data(SRC, TGT, "validation")

"""---
## 5. Metrics
"""

import sacrebleu
from jiwer import wer as calc_wer

try:
    from comet import download_model, load_from_checkpoint
    comet_model = load_from_checkpoint(download_model("Unbabel/wmt22-comet-da"))
    COMET_OK = True; print("COMET ready!")
except: COMET_OK = False; print("COMET not available")

def compute_metrics(hyps, refs, srcs=None):
    v = [(h, r, s or "") for h, r, s in zip(hyps, refs, srcs or [""] * len(hyps)) if h.strip() and r.strip()]
    if not v: return {"bleu": 0, "chrf": 0, "comet": 0, "wer": 1}
    h, r, s = [x[0] for x in v], [x[1] for x in v], [x[2] for x in v]
    res = {"bleu": sacrebleu.corpus_bleu(h, [r]).score, "chrf": sacrebleu.corpus_chrf(h, [r]).score,
           "wer": sum(calc_wer(ref, hyp) for ref, hyp in zip(r, h)) / len(h)}
    if COMET_OK and srcs:
        try: res["comet"] = comet_model.predict([{"src": a, "mt": b, "ref": c} for a, b, c in zip(s, h, r)], batch_size=8, gpus=1).system_score
        except: res["comet"] = 0
    else: res["comet"] = 0
    return res

"""---
## 6. Inference
"""

def translate(audio, sr, tgt_lang, return_audio=False):
    """SeamlessM4T inference.

    - **Text metrics (BLEU/ChrF/COMET/WER)** need text output only.
    - **Speech output** is optional (for MOS).

    Important:
    - Odia text code is `ory`.
    - `ory` is NOT supported for *speech synthesis* (vocoder/t2u) in SeamlessM4T-medium.
    - Some Transformers versions still validate `tgt_lang` against the speech-synthesis list even when
      `generate_speech=False`. For those cases we use a workaround: pass a safe `tgt_lang` (e.g. `hin`)
      but force the text decoder language with `forced_bos_token_id`.

    This version also avoids common dtype crashes (Float/Half) by:
    - casting float tensors to the model dtype
    - running generate() under autocast on CUDA
    """

    if sr != 16000:
        audio = torchaudio.functional.resample(torch.tensor(audio), sr, 16000).numpy()

    # Processor API differs across transformer versions: try `audio=` then fallback to `audios=`
    try:
        inputs = processor(audio=audio, return_tensors="pt", sampling_rate=16000)
    except TypeError:
        inputs = processor(audios=audio, return_tensors="pt", sampling_rate=16000)

    model_dtype = next(model.parameters()).dtype
    inputs = {
        k: (v.to(device=device, dtype=model_dtype) if torch.is_tensor(v) and v.is_floating_point() else v.to(device))
        for k, v in inputs.items()
    }

    use_amp = (device.type == "cuda" and model_dtype == torch.float16)

    tgt = LANG[tgt_lang]

    # Speech output guard
    if return_audio and tgt == "ory":
        raise ValueError("Speech output is not supported for tgt_lang='ory' on SeamlessM4T-medium. Use generate_speech=False.")


    t2u_map = getattr(model.generation_config, "t2u_lang_code_to_id", {}) or {}
    vocoder_map = getattr(model.generation_config, "vocoder_lang_code_to_id", {}) or {}
    text_map = getattr(model.generation_config, "text_decoder_lang_to_code_id", {}) or {}

    if (not return_audio) and (tgt not in t2u_map) and (tgt in text_map):
        if not t2u_map:
            raise ValueError("This model does not expose t2u_lang_code_to_id; can't patch tgt_lang validation.")
        dummy_t2u = t2u_map.get("hin") or next(iter(t2u_map.values()))
        t2u_map[tgt] = dummy_t2u
        model.generation_config.t2u_lang_code_to_id = t2u_map

        if vocoder_map and (tgt not in vocoder_map):
            dummy_vocoder = vocoder_map.get("hin") or next(iter(vocoder_map.values()))
            vocoder_map[tgt] = dummy_vocoder
            model.generation_config.vocoder_lang_code_to_id = vocoder_map

    from contextlib import nullcontext
    amp_ctx = torch.amp.autocast("cuda") if use_amp else nullcontext()

    # Generation params to prevent repetitive/degenerate output
    gen_kwargs = {
        "tgt_lang": tgt,
        "repetition_penalty": 1.5,      # Penalize repeated tokens
        "no_repeat_ngram_size": 3,      # Prevent 3-gram repetition
        "max_new_tokens": 256,
    }

    with torch.no_grad(), amp_ctx:
        if return_audio:
            out = model.generate(**inputs, **gen_kwargs)
        else:
            out = model.generate(**inputs, **gen_kwargs, generate_speech=False)

    if return_audio:
        audio_out = out[0] if isinstance(out, (list, tuple)) else out
        return audio_out.detach().cpu().numpy().squeeze(), model.config.sampling_rate

    seq = out.sequences if hasattr(out, "sequences") else out
    if hasattr(seq, "detach"):
        seq = seq.detach()
    if hasattr(seq, "cpu"):
        seq = seq.cpu()

    hyp = processor.tokenizer.batch_decode(seq, skip_special_tokens=True)[0]


    if tgt == "ory" and not globals().get("_DBG_TRANSLATE_ONCE", False):
        globals()["_DBG_TRANSLATE_ONCE"] = True
        t2u_map2 = getattr(model.generation_config, "t2u_lang_code_to_id", {}) or {}
        voc_map2 = getattr(model.generation_config, "vocoder_lang_code_to_id", {}) or {}

        seq_head = None
        raw_dec = None
        try:
            s0 = seq[0]
            if hasattr(s0, "detach"):
                s0 = s0.detach().cpu()
            seq_head = s0[:10].tolist()
            raw_dec = processor.tokenizer.decode(s0[:30], skip_special_tokens=False)[:200]
        except Exception:
            pass

        if "_dbglog" in globals():
            _dbglog(
                "H4",
                "seamless_m4t_finetuning_colab.ipynb:translate",
                "translate_output",
                {
                    "tgt": tgt,
                    "empty": (not hyp.strip()),
                    "len": int(len(hyp.strip())),
                    "seq_head": seq_head,
                    "raw_dec": raw_dec,
                    "t2u_has": bool(tgt in t2u_map2),
                    "vocoder_has": bool(tgt in voc_map2),
                },
            )


    return hyp

def evaluate(samples, tgt_lang, n=30):
    hyps, refs, srcs = [], [], []
    for s in tqdm(samples[:n], desc="Eval"):
        try:
            hyps.append(translate(s["audio"], s["sr"], tgt_lang))
            refs.append(s["tgt_text"]); srcs.append(s["src_text"])
        except Exception as e: print(f"Err: {e}")
    return compute_metrics(hyps, refs, srcs), hyps, refs, srcs

"""---
## 7. Baseline
"""

print("="*50 + "\nBASELINE\n" + "="*50)
baseline, b_h, b_r, b_s = evaluate(val_data, TGT, n=20)
print(f"BLEU: {baseline['bleu']:.2f}, ChrF: {baseline['chrf']:.2f}, COMET: {baseline['comet']:.4f}, WER: {baseline['wer']:.4f}")
for i in range(min(2, len(b_h))): print(f"\n[{i+1}] Ref: {b_r[i][:50]}... | Gen: {b_h[i][:50]}...")

"""---
## 8. Finetuning
"""

from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW

class DS(Dataset):
    def __init__(self, data, processor, tgt):
        self.data = data
        self.processor = processor
        self.tgt = tgt
        self.tgt_lang = LANG[tgt]
        self.lang_token = f"__{self.tgt_lang}__"
        self.lang_id = processor.tokenizer.convert_tokens_to_ids(self.lang_token)
        self._dbg_logged_item = False


        if "_dbglog" in globals():
            _dbglog(
                "H1",
                "seamless_m4t_finetuning_colab.ipynb:DS.__init__",
                "lang_token_lookup",
                {
                    "tgt_lang": self.tgt_lang,
                    "lang_token": self.lang_token,
                    "lang_id": int(self.lang_id),
                    "unk_id": int(processor.tokenizer.unk_token_id),
                },
            )


        if self.lang_id == processor.tokenizer.unk_token_id:
            print(f"WARNING: tokenizer is missing {self.lang_token}; labels may be misaligned.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        s = self.data[i]
        audio = s["audio"]
        if s["sr"] != 16000:
            audio = torchaudio.functional.resample(torch.tensor(audio), s["sr"], 16000).numpy()
        if len(audio) > 15 * 16000:
            audio = audio[:15 * 16000]

        try:
            inp = self.processor(audio=audio, return_tensors="pt", sampling_rate=16000)
        except TypeError:
            inp = self.processor(audios=audio, return_tensors="pt", sampling_rate=16000)

        # Ensure the tokenizer knows which language to target
        self.processor.tokenizer.tgt_lang = self.tgt_lang
        lab = self.processor.tokenizer(
            text_target=s["tgt_text"], return_tensors="pt", truncation=True, max_length=256
        )
        ids = lab["input_ids"].squeeze()
        if ids.dim() == 0:
            ids = ids.unsqueeze(0)


        dec_start_id = getattr(getattr(globals().get("model", None), "generation_config", None), "decoder_start_token_id", None)
        if dec_start_id is None:
            dec_start_id = getattr(self.processor.tokenizer, "eos_token_id", None)

        orig_first = int(ids[0].item()) if ids.numel() > 0 else None
        orig_second = int(ids[1].item()) if ids.numel() > 1 else None
        did_drop_start = False
        did_prepend = False

        # If tokens look like: [</s>, __ory__, ...] -> drop the initial </s>
        if (
            dec_start_id is not None
            and self.lang_id != self.processor.tokenizer.unk_token_id
            and ids.numel() > 1
            and ids[0].item() == int(dec_start_id)
            and ids[1].item() == int(self.lang_id)
        ):
            ids = ids[1:]
            did_drop_start = True


        if (
            self.lang_id != self.processor.tokenizer.unk_token_id
            and ids.numel() > 0
            and ids[0].item() != self.lang_id
        ):
            ids = torch.cat([torch.tensor([self.lang_id], dtype=ids.dtype), ids], dim=0)
            did_prepend = True

        if not self._dbg_logged_item:
            self._dbg_logged_item = True
            new_first = int(ids[0].item()) if ids.numel() > 0 else None

            if "_dbglog" in globals():
                _dbglog(
                    "H2",
                    "seamless_m4t_finetuning_colab.ipynb:DS.__getitem__",
                    "label_first_token_check",
                    {
                        "index": int(i),
                        "orig_first": orig_first,
                        "orig_second": orig_second,
                        "new_first": new_first,
                        "did_drop_start": did_drop_start,
                        "did_prepend": did_prepend,
                        "len": int(ids.numel()),
                        "lang_id": int(self.lang_id),
                        "dec_start_id": int(dec_start_id) if dec_start_id is not None else None,
                        "unk_id": int(self.processor.tokenizer.unk_token_id),
                    },
                )


        return {"input_features": inp["input_features"].squeeze(), "labels": ids}


def collate(batch):
    mf = max(b["input_features"].shape[-1] for b in batch)
    ml = max(b["labels"].shape[0] for b in batch)
    feats, labs = [], []
    for b in batch:
        f, l = b["input_features"], b["labels"]
        if (pf := mf - f.shape[-1]) > 0:
            f = torch.nn.functional.pad(f, (0, pf))
        if (pl := ml - l.shape[0]) > 0:
            l = torch.nn.functional.pad(l, (0, pl), value=-100)
        feats.append(f)
        labs.append(l)
    return {"input_features": torch.stack(feats), "labels": torch.stack(labs)}

def finetune(train, val, tgt, epochs=2, lr=1e-5, eval_every=50, save_best=True, save_final=True, save_initial_best=False):
    print("\n" + "=" * 50 + "\nFINETUNING\n" + "=" * 50)
    print(f"[ckpt] save_initial_best={save_initial_best} save_best={save_best} save_final={save_final}")


    t2u_map = getattr(model.generation_config, "t2u_lang_code_to_id", {}) or {}
    vocoder_map = getattr(model.generation_config, "vocoder_lang_code_to_id", {}) or {}
    text_map = getattr(model.generation_config, "text_decoder_lang_to_code_id", {}) or {}
    tgt_code = LANG[tgt]
    if tgt_code not in t2u_map and tgt_code in text_map:
        if t2u_map:
            t2u_map[tgt_code] = t2u_map.get("hin") or next(iter(t2u_map.values()))
            model.generation_config.t2u_lang_code_to_id = t2u_map
            print(f"Patched t2u_lang_code_to_id for {tgt_code}")
        if vocoder_map and tgt_code not in vocoder_map:
            vocoder_map[tgt_code] = vocoder_map.get("hin") or next(iter(vocoder_map.values()))
            model.generation_config.vocoder_lang_code_to_id = vocoder_map

    train_dl = DataLoader(DS(train, processor, tgt), batch_size=1, shuffle=True, collate_fn=collate)
    val_dl = DataLoader(DS(val[:20], processor, tgt), batch_size=1, collate_fn=collate)
    print(f"Train: {len(train_dl)}, Val: {len(val_dl)}")


    frozen_count = 0
    trainable_count = 0
    for name, param in model.named_parameters():
        # Freeze everything except text decoder and lm_head
        if "text_decoder" in name or "lm_head" in name:
            param.requires_grad = True
            trainable_count += 1
        else:
            param.requires_grad = False
            frozen_count += 1
    print(f"Frozen {frozen_count} params, training {trainable_count} params (text_decoder + lm_head only)")


    if "_dbglog" in globals():
        _dbglog(
            "H6",
            "seamless_m4t_finetuning_colab.ipynb:finetune",
            "param_freeze_status",
            {"frozen": frozen_count, "trainable": trainable_count},
        )


    trainable_params = [p for p in model.parameters() if p.requires_grad]
    opt = AdamW(trainable_params, lr=lr)

    model_dtype = next(model.parameters()).dtype
    use_amp = device.type == "cuda"

    # GradScaler is unsafe when the model itself is already in fp16, so disable it there
    use_scaler = use_amp and (model_dtype == torch.float32)
    scaler = torch.amp.GradScaler("cuda") if use_scaler else None

    lang_token = f"__{tgt_code}__"
    lang_id = processor.tokenizer.convert_tokens_to_ids(lang_token)


    if "_dbglog" in globals():
        _dbglog(
            "H3",
            "seamless_m4t_finetuning_colab.ipynb:finetune",
            "finetune_start",
            {
                "tgt": tgt,
                "tgt_code": tgt_code,
                "lang_token": lang_token,
                "lang_id": int(lang_id),
                "unk_id": int(processor.tokenizer.unk_token_id),
                "t2u_has": bool(tgt_code in t2u_map),
                "vocoder_has": bool(tgt_code in vocoder_map),
            },
        )


    best_dir = f"{SAVE_DIR}/best"
    os.makedirs(best_dir, exist_ok=True)

    model.eval()
    with torch.no_grad():
        init_vl = 0.0
        for vb in val_dl:
            vfeats = vb["input_features"].to(device=device, dtype=model_dtype)
            vlabs = vb["labels"].to(device)
            if use_amp:
                with torch.amp.autocast("cuda"):
                    vout = model(input_features=vfeats, labels=vlabs, tgt_lang=LANG[tgt])
            else:
                vout = model(input_features=vfeats, labels=vlabs, tgt_lang=LANG[tgt])
            init_vl += vout.loss.item()
        init_vl /= len(val_dl)

    if save_initial_best:
        print("[ckpt] Saving initial best checkpoint...")
        _t0 = time.time()
        model.save_pretrained(best_dir)
        processor.save_pretrained(best_dir)
        print(f"[ckpt] Saved initial best to {best_dir} in {time.time()-_t0:.1f}s")
    else:
        print(f"Initial val_loss={init_vl:.4f} (not saving initial checkpoint)")

    model.train()
    best, step = init_vl, 0


    first_batch = next(iter(train_dl))
    lbl = first_batch["labels"][0]
    lbl = lbl[lbl != -100]
    print(f"DEBUG: sample label IDs: {lbl.tolist()}")
    print(f"DEBUG: sample label text: {processor.tokenizer.decode(lbl)}")
    unk_id = processor.tokenizer.unk_token_id
    if (lang_id != unk_id and lbl.numel() > 0 and lbl[0].item() == lang_id):
        print("DEBUG: language token present at start of labels.")
    elif lang_id == unk_id:
        print(f"DEBUG ERROR: Language token '{lang_token}' not found in tokenizer vocabulary!")
    else:
        print(f"DEBUG WARNING: language token missing from labels! Expected {lang_id}, got {lbl[0].item() if lbl.numel() > 0 else 'empty'}")


    if "_dbglog" in globals():
        _dbglog(
            "H2",
            "seamless_m4t_finetuning_colab.ipynb:finetune",
            "first_label_prefix",
            {
                "lbl_head": lbl[:6].detach().cpu().tolist() if lbl.numel() > 0 else [],
                "lang_id": int(lang_id),
                "unk_id": int(unk_id),
            },
        )

    if "_dbglog" in globals():
        try:
            dec_in = model.prepare_decoder_input_ids_from_labels(first_batch["labels"].to(device))
            dec_head = dec_in[0][:5].detach().cpu().tolist()
            _dbglog(
                "H2",
                "seamless_m4t_finetuning_colab.ipynb:finetune",
                "decoder_input_ids_head",
                {
                    "dec_head": dec_head,
                    "lang_id": int(lang_id),
                    "unk_id": int(unk_id),
                    "dec_token1": int(dec_head[1]) if len(dec_head) > 1 else None,
                    "token1_matches_lang": bool(len(dec_head) > 1 and int(dec_head[1]) == int(lang_id)),
                },
            )
        except Exception as e:
            _dbglog(
                "H2",
                "seamless_m4t_finetuning_colab.ipynb:finetune",
                "decoder_input_ids_error",
                {"error": str(e)},
            )

    for ep in range(epochs):
        for batch in tqdm(train_dl, desc=f"Ep{ep+1}"):
            opt.zero_grad(set_to_none=True)
            try:
                input_feats = batch["input_features"].to(device=device, dtype=model_dtype)
                labels = batch["labels"].to(device)

                if use_amp:
                    with torch.amp.autocast("cuda"):
                        out = model(input_features=input_feats, labels=labels, tgt_lang=LANG[tgt])
                else:
                    out = model(input_features=input_feats, labels=labels, tgt_lang=LANG[tgt])

                if step == 0:

                    if "_dbglog" in globals():
                        try:
                            _dbglog(
                                "H5",
                                "seamless_m4t_finetuning_colab.ipynb:finetune",
                                "first_train_loss",
                                {
                                    "loss": float(out.loss.detach().cpu().item()),
                                    "is_nan": bool(torch.isnan(out.loss).item()),
                                    "is_inf": bool(torch.isinf(out.loss).item()),
                                    "dtype": str(out.loss.dtype),
                                },
                            )
                        except Exception as e:
                            _dbglog(
                                "H5",
                                "seamless_m4t_finetuning_colab.ipynb:finetune",
                                "first_train_loss_error",
                                {"error": str(e)},
                            )


                if use_scaler:
                    scaler.scale(out.loss).backward()
                    scaler.unscale_(opt)
                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                    scaler.step(opt)
                    scaler.update()
                else:
                    out.loss.backward()
                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                    opt.step()

                step += 1
            except RuntimeError as e:
                if "memory" in str(e):
                    torch.cuda.empty_cache()
                    continue
                raise

            if step % eval_every == 0:
                model.eval()
                with torch.no_grad():
                    vl = 0.0
                    for vb in val_dl:
                        vfeats = vb["input_features"].to(device=device, dtype=model_dtype)
                        vlabs = vb["labels"].to(device)
                        if use_amp:
                            with torch.amp.autocast("cuda"):
                                vout = model(input_features=vfeats, labels=vlabs, tgt_lang=LANG[tgt])
                        else:
                            vout = model(input_features=vfeats, labels=vlabs, tgt_lang=LANG[tgt])
                        vl += vout.loss.item()
                    vl /= len(val_dl)
                # Handle NaN val_loss (gradient explosion symptom)
                if torch.isnan(torch.tensor(vl)) or torch.isinf(torch.tensor(vl)):
                    print(f"Step {step}: val_loss=NaN/Inf (skipping checkpoint save)")
                    model.train()
                    continue
                print(f"Step {step}: val_loss={vl:.4f}")
                if vl < best:
                    best = vl
                    if save_best:
                        print("[ckpt] New best! Saving BEST checkpoint...")
                        _t0 = time.time()
                        model.save_pretrained(f"{SAVE_DIR}/best")
                        processor.save_pretrained(f"{SAVE_DIR}/best")
                        print(f"[ckpt] Saved BEST in {time.time()-_t0:.1f}s")
                    else:
                        print("[ckpt] New best! (save_best=False, skipping save)")
                model.train()

    final_dir = f"{SAVE_DIR}/final"
    os.makedirs(final_dir, exist_ok=True)
    if save_final:
        print("[ckpt] Saving FINAL checkpoint (this may take 1-2 min)...")
        _t0 = time.time()
        model.save_pretrained(final_dir)
        processor.save_pretrained(final_dir)
        print(f"[ckpt] Saved FINAL checkpoint to: {final_dir} in {time.time()-_t0:.1f}s")
    else:
        print("[ckpt] Skipping FINAL checkpoint save (save_final=False)")

    print(f"Done! Best: {best:.4f}")
    return model

ft = finetune(train_data, val_data, TGT, epochs=5, eval_every=50, save_best=False, save_final=False, save_initial_best=False)

"""---
## 9. Post-Finetuning
"""

# Load finetuned weights for evaluation
best_dir = f"{SAVE_DIR}/best"
final_dir = f"{SAVE_DIR}/final"

# Load finetuned checkpoint (prefer in-memory model to avoid loading NaN-corrupted final checkpoint)
if "ft" in globals() and isinstance(ft, SeamlessM4TModel):
    print("Using in-memory finetuned model: ft")
    model = ft
elif os.path.isdir(final_dir):
    print(f"Loading FINAL checkpoint from: {final_dir}")
    model = SeamlessM4TModel.from_pretrained(final_dir, torch_dtype=DTYPE).to(device)
elif os.path.isdir(best_dir):
    print(f"Loading BEST checkpoint from: {best_dir}")
    model = SeamlessM4TModel.from_pretrained(best_dir, torch_dtype=DTYPE).to(device)
elif "ft" in globals() and isinstance(ft, SeamlessM4TModel):
    print("Using in-memory finetuned model: ft")
    model = ft
else:
    print("\nWARNING: No finetune checkpoint found; using current in-memory `model`.")


from transformers import GenerationConfig
try:
    base_gen_config = GenerationConfig.from_pretrained(MODEL_ID)
    model.generation_config = base_gen_config
    print("Restored generation_config from base model")
except Exception as e:
    print(f"Warning: Could not restore generation_config: {e}")


t2u_map = getattr(model.generation_config, "t2u_lang_code_to_id", {}) or {}
vocoder_map = getattr(model.generation_config, "vocoder_lang_code_to_id", {}) or {}
if t2u_map and "ory" not in t2u_map:
    t2u_map["ory"] = t2u_map.get("hin", next(iter(t2u_map.values())))
    model.generation_config.t2u_lang_code_to_id = t2u_map
    print("Patched t2u_lang_code_to_id for ory")
if vocoder_map and "ory" not in vocoder_map:
    vocoder_map["ory"] = vocoder_map.get("hin", next(iter(vocoder_map.values())))
    model.generation_config.vocoder_lang_code_to_id = vocoder_map

model.eval()

print("="*50 + "\nPOST-FINETUNING\n" + "="*50)
print(f"Pair: {SRC}->{TGT} | tgt_lang={LANG[TGT]}")
print(f"val_data: {len(val_data)} samples")

# Quick sanity check with translate()
try:
    s0 = val_data[0]
    hyp0 = translate(s0["audio"], s0["sr"], TGT)
    print(f"\n[Sanity] REF: {s0['tgt_text'][:120]}...")
    print(f"[Sanity] HYP: {hyp0[:120] if hyp0 else '(empty)'}...")
except Exception as e:
    import traceback
    print(f"\n[Sanity] translate() failed: {e}")
    traceback.print_exc()

finetuned, ft_h, ft_r, ft_s = evaluate(val_data, TGT, n=20)
non_empty = sum(1 for h in ft_h if isinstance(h, str) and h.strip())
print(f"Non-empty hypotheses: {non_empty}/{len(ft_h)}")

print(f"BLEU: {finetuned['bleu']:.2f}, ChrF: {finetuned['chrf']:.2f}, COMET: {finetuned['comet']:.4f}, WER: {finetuned['wer']:.4f}")

print("\nCOMPARISON")
print(f"{'Metric':<8} {'Base':>10} {'Fine':>10} {'Delta':>10}")
for m in ["bleu", "chrf", "comet", "wer"]:
    print(f"{m.upper():<8} {baseline[m]:>10.2f} {finetuned[m]:>10.2f} {finetuned[m]-baseline[m]:>+10.2f}")